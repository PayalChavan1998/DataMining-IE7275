{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fb06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10f56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = '/Users/payalchavan/Documents/Data Mining_Assignments/celeba/img_align_celeba'\n",
    "attributes_file = '/Users/payalchavan/Documents/Data Mining_Assignments/celeba/list_attr_celeba.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attributes\n",
    "attributes = {}\n",
    "with open(attributes_file, 'r') as file:\n",
    "    file.readline()  # Skip the header line (number of samples)\n",
    "    attribute_names = file.readline().strip().split()  # Get the attribute names\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        attributes[parts[0]] = [1 if int(p) == 1 else 0 for p in parts[1:]]\n",
    "\n",
    "# Find the index of the \"Wearing a hat\" attribute\n",
    "Wearing_hat_index = attribute_names.index('Wearing_Hat')\n",
    "\n",
    "# Load images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for filename in sorted(os.listdir(image_folder)):\n",
    "    if filename.lower().endswith('.jpg'):\n",
    "        # Read and preprocess the image\n",
    "        img = cv2.imread(os.path.join(image_folder, filename))\n",
    "        img = cv2.resize(img, (64, 64))  # Resize to 64x64\n",
    "        img = img / 255.0  # Normalize pixel values\n",
    "        images.append(img)\n",
    "\n",
    "        # Extract the \"Wearing_hat_index\" attribute as the label\n",
    "        labels.append(attributes[filename][Wearing_hat_index])\n",
    "\n",
    "images = np.array(images, dtype='float32')\n",
    "labels = np.array(labels, dtype='int')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),  \n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c880eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy of the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy of the model: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c466f",
   "metadata": {},
   "source": [
    "The overall accuracy of a model is determined by the ratio of total correct predictions to the total number of predictions (both correct and incorrect). The formula for accuracy is as follows:\n",
    "[ \\text{Accuracy} = \\frac{\\text{Total correct predictions}}{\\text{Total correct predictions} + \\text{Total incorrect predictions}} ]\n",
    "This accuracy value represents the model’s performance across the entire dataset or batch. It provides insight into how well the model generalizes to unseen examples. Remember that achieving high accuracy is essential, but it’s equally crucial to evaluate other metrics and consider the context of the problem domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Determine the number of correct and incorrect predictions\n",
    "correct_predictions = np.sum(predicted_labels == y_test)\n",
    "incorrect_predictions = np.sum(predicted_labels != y_test)\n",
    "\n",
    "print(f\"Total correct predictions: {correct_predictions}\")\n",
    "print(f\"Total incorrect predictions: {incorrect_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the history data\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "plt.plot(epochs, acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c418a",
   "metadata": {},
   "source": [
    "The trend of increasing accuracy during training epochs indicates effective learning from the training data. However, the gap between training and validation accuracy hints at potential overfitting, where the model learns specific patterns from the training set that may not apply well to new data.\n",
    "\n",
    "The minor fluctuations in validation metrics in later epochs suggest the early stages of overfitting. This can be addressed by introducing techniques like dropout layers or regularization to prevent the model from memorizing the training data too closely.\n",
    "\n",
    "The final test accuracy is a crucial metric indicating how well the model generalizes to new, unseen data. Achieving an accuracy of over 91% is commendable, showcasing the model's ability to extract meaningful features from face images to predict smiles accurately.\n",
    "\n",
    "To enhance the model further, exploring data augmentation methods can diversify the training set, aiding the model in better generalization. Experimenting with different architectures or hyperparameters could also lead to performance enhancements.\n",
    "\n",
    "The final test accuracy is a critical metric. Achieving an accuracy of over 91% is commendable. It demonstrates the model’s ability to extract meaningful features from face images and predict smiles accurately.\n",
    "However, always validate the model’s performance on unseen data to ensure robustness.\n",
    "Further Model Enhancement:\n",
    "To enhance the model further:\n",
    "Data Augmentation: Explore data augmentation methods to diversify the training set. Augmented data can help the model generalize better.\n",
    "Architecture and Hyperparameters: Experiment with different architectures or hyperparameters. Fine-tuning these aspects could lead to performance improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
